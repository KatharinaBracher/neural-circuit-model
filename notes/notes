# Reading

## Narain, Jazayeri et al. (2018): A cerebellar mechanism for learning prior distributions of time intervals
<details>
<summary> Model with 3 components, based on Bayesian estimate 
</summary>
anatomical motivated model (GC, PC, CF and DN in cerebellum) of learning prior on circuit level

Bayesian estimator assumes:
- uniform prior distribution perfectly learned by observer
- scalar variability 
- BLS estimate - expected value of posterior (vs. max likelihood - peak of posterior)
- also shown with Gaussian prior (more realistic, a learned prior can only be as precise as the likelihood, which also is Gaussian)

BLS estimator can be formulated as a deterministic nonlinear function

To learn prior model needs: 
- basis set of activity (Gaussian) that has a decay in amplitude (increase in std) to account for scalar variability
- learning rule (LTD and LTP), where LTD is suppressing PC activity at the time of set (in RSG task) and LTP as restoring force to a basis level of w, here the prior is encoded 
(should be same the other way round, in cortex)
- integrator to estimate (not necessary as information already in learned activity)

Open questions: 
- mapping from DN activity to estimate 
- reproduction of the learned interval
- population activity of basis set
</details>

## Bi and Zhou (2020): Understanding the computation of time using neural network models. 
<details>
<summary> Recurrent net, trained in a supervised fashion 
</summary>
Recurrent neural network structure: 

- 256 units
- soft plus activation
- recurrent inputs and all to all connection with learnable weights
- noise inputs
- input units depending on task, set weights
- output unit(s) with set weights, number of units depending on target

**Training**: 
- Cost function (MSE), sum over output units
- Adam optimizer

**Population activity very similar to experimental findings**:
- perceived time: stereotypical dynamic trajectories
- delay period, maintain time: attractor dynamics in a complementary monotonic coding scheme (mapping of manifold to another, different speed but same space between trajectories)
- production by scaling evolution speed.
- temporal and nontemporal information is coded in orthogonal subspaces with small mixed variance (interesting analyses)
</details>

**Thoughts**: 
- Is there are more biological motivated way to train the network? (LTD, LTP) and not so DL motivated, maybe reinforcement learning? Learning in biological plausible way
	- is the network still able to learn with a no so powerful learning rule?
- what about inh and exc weights? need restriction that a unit can only project neg. or pos.



## Egger, Le and Jazayeri (2020): A neural circuit model for human sensorimotor timing
<details>
<summary> Connecting thresholds and slopes with an inh/exc circuit 
</summary>

**absolute and predictive timing mechanisms**: 
- absolute: integrates ticks from central clock, activity increases monotonically, rate of increase does not depend on interval, output levels serve as continuous 
- predictive timing mechanism: rate of increase is adjusted so that the output reaches the same point after an anticipated interval, rate of increase has to be set

**Circuit**: 
- biological findings suggest change of rate to reach threshold 
- circuit consists of 3 units u,v,y (u, v with sigmoidal activation function) that each represent the average activity of a population of neurons
- module that translates different input $I$ to different slops: shared input to $v$ and $u$, which inhibit each other and project inh and exc respectively on $y$
  - identical shared excitatory input
  - symmetric mutual inhibition
- dynamical regimes (depend on parameter)
  - 3 FP (stable, unstable u=v, stable)
  - u (v) dominating v (u), evolve towards one of the stable FP (depending on initial conditions ramping up or ramping down)
  - leads to ramp-like behavior in y which rate is inversely proportional to input I
- initial conditions of u and v
  - mean reproduction robust, can be generalized to different dynamical regimes
- $y$ reaches threshold $y_0$, depending on slope (input $I$) the output is at different times
  - adjusting I means flexible adjusting movement initiation time (later coupling strength of I to interval time)

> speed control mechanism can be understood by analyzing system dynamics in the phase plane of u and v

**Parameter**:
- regimes of I (shifting nullclines): 
  - intermediate I, 2 FP, higher I makes the system go slower
  - low I (\<0.5), 2 FP, realtion of I and speed inverted, higher I makes the system go faster
  - high I (\>1), only 1 FP on u=v line, systems moves toward this line, requires a lower bound to be set
    - Although the circuit successfully matches IPI to the ISI in this regime, it fails to generate parallel neural trajectories in the uâˆ’v plane.
- noise level maximal 25% of input: monotonic increase of standard deviation of reproductions
  - magnitude of bias determined by noise
  - tuned to achieve a level of bias-variance trade-off that is inversely related with the inherent noise and the operative cost function
- initial delay serving as an initial estimate before any feedback
  - independent of noise level and number of stimuli
- K memory parameter
  - optimal K decreases with increasing noise
  - optimal K decreases with increasing number samples (reduced weight given to each error, allows model to integrate across inputs)
  - central role in determining the bias-variance trade-off (magnitude of bias determined by noise)

**Model that produces multiple actions and adapts I**:
- need to introduce reset mechanism to reset u and v for a new production ('new ramp'): transient pulse (1 bin)
  - neurobiological perspective: reset signal can be generated by the corollary discharge associated with the movement command or by sensory feedback triggered by the movement
  - implementation: integrating summed activity of y and tonically firing inhibitory unit y_0 into input unit, when stimulus (flash)
- need to introduce update mechanism for I to time inputs according to a stimulus
  - updating I dynamically at a rate proportional to the error signal $y_s - y_0$, $y_s$ is matched to $y_0$ at time of stimulus
- when there is a stimulus u and v are also reset
	+ we can implement the update instantaneous as $\Delta y$
- after adjusting I the motor production module outputs $y_p$ (and also reset)
	- is the motor production module necessary? the prev. module is outputting I in which the interval is already encoded

</details>

**Thoughts**:
- we don't need the slope, the interval is encoded in I (rate) and is just translated in slope with the exc/inh circuit such that high rates have slow slope and low rates have steep slope
- I functions as a threshold (absolute mechanism) and encodes the interval, slope is just the next step to reproduce the interval 

**Steps**: 
- implement and reproduce fig 3 with the update mechanism of I to encode stimulus interval 
- look at u and v as populations, is there more information in the neural trajectories?
- what about y?
- is there a biological more plausible way to update I?
- How can we learn the weights in u and v populations such that they can react on changes of stimuli ranges/ times?



## Simen, Holmes et al. (2011): A Model of Interval Timing by Neural Integration
<details>
<summary> Stochastic ramp and trigger circuit motivated by nonhomogeneous Poisson spike rate (and shot noise population model); can be reduced to a opponent poisson diffusion model 
</summary>

Stochastic ramp and trigger timing circuit
- bistable start switch with a connections to a number of clock pulse generator neurons. Number of active generator neurons determines the strength of drive to the integrator (ramp), the drive then determines the rate of ramp of the integrator. A bistable trigger switch is turned on when a fixed threshold is passed.
- 2 learning processes update the number of active units in the clock pulse generator that then directly influences the rate of the integrator 
</details>

**Thoughts**: 
<details>
<summary> Is it the same model as in Egger 2020? 
</summary>
Either the number of units is updated that determines the drive of the integrator (the more units the steeper the rate the shorter the interval); the learning rate incorporates the swap, the longer the interval, the smaller the number of active units has to be

or the input I is updated (the stronger I the smaller the slope the longer the interval); the dynamics in u,v incorporates the swap from strong input to small slope, learning is just a difference between 
</details>

- Translation of stochastic ramp and trigger timing circuit to diffusion model, with keeping parameters 
- Thresholds are implemented as bistable switches 
 
## Thurley (2016): Magnitude Estimation with Noisy Integrators Linked by an Adaptive Reference
<details>
<summary> Drift diffusion model with two processes (measurement and reproduction) and adaptive threshold (depending on memory parameter a and previous thresholds).
</summary>

**Psychophysical characteristics of magnitude estimation**: 
- Regression effect: over a range of stimuli, small stimuli are overestimated whereas large stimuli are underestimated
- Range effect: regression becomes more pronounced for ranges that comprise larger stimulus values
- Scalar variability: errors monotonically increase with the size of the stimulus
- Sequential effect: the estimate of the stimulus in a particular trial is affected by the previous trial

Effects result from an optimal strategy when noisy estimates are made about stimuli that itself depend on the statistics of the environment.

**Model**:
- Measurement (Reproduction): drift process with rate $A_m$ ($A_r$) corrupted by noise $\sigma_m$ ($\sigma_r$), dynamics described by stochastic differential equation 
  - Measurement ends with stimulus, integration differential equation yields measurement: $m_T$ -> incorporated into the threshold of reproduction
  - Measurement: drift-diffusion process that lasts a fixed time -> Gaussian distribution of level reached
  - Reproduction: drift-diffusion process with fixed threshold -> Inverse Gaussian distribution of times that it takes to reach threshold
- Threshold: depends on previous thresold (weighted by $1-a$) and measurement $m_T$ (weighted by $a$)
  -  memory weight a has an immediate impact on the relation between stimulus and response
  - history of thresholds from previous trials, serving as an internal reference
- Formula for average reproduction $\bar{r}_T$ and its Bias and expected value yields full characterization of the model linking the stimuli $T$ to their reproduced values $\bar{r}_T$
- Error: $MSE_r = [(r_T-T)^2]$
  - memory a of the system can be adapted to minimize the mean squared error
- Optimizing for $a$ shows dependency on internal processing (drift ratio for m and r, internal noise $\sigma_m$ (not $\sigma_r$)) and external variability (mean and variance of stimuli).
  - Optimality predicts range and regression effects

**Fitting parameter and predictions**:
- fitting $A$, $\sigma$ to experimental data and optimize for $a$
- make predictions about $a$ based on optimality (with other parameter fitted)
- changing stimulus range predicts a different optimal $a$ - can be tested
- Predictions of the model
  - Reproduced magnitudes should depend on the stimulus distribution (Stimulus distributions with the same mean but larger variances should result in less regression)
  - Regression to the mean should depend on the discrimination abilities of the individual (internal noise $\sigma_m$)
  - Seldom stimuli with a low probability of occurrence and with a magnitude way below or way above the stimulus distribution, should not influence the internal reference
  - For strong regression the convergence dynamics of the reference should be much slower then for subjects showing weak regression
</details>

**Thoughts**: 
- find parameter range, what do the parameters mean? What happens when changing parameters?
- Optimality and biological relevance


### Wang, Jazayeri et al.: Flexible timing by temporal scaling of cortical responses
<details>
<summary>

Temporal scaling of activity neuron subpopulations in MFC and downstream Caudate, but not in upstream thalamic neurons. \
Recurrent neural net revealed that scaling emerges from nonlinearities in network and that degree of scaling is controlled by strength of external input. \
Introduction of two neuron model for general mechanism for translating input to the effective time constant of the dynamic.
</summary>

**PCA and SCA on neural data**
- scaling index on PCs show parts of scaling and non-scaling (-> scaling subspace)
- linear dimensionality reduction that finds axis of strongest scaling
- the higher the scaling index the higher the explained varince

**RNN**:
- input of cue with different magnitude (transient or continuous) that indicates length of interval to be reproduced and set indicating the start of the interval
- output was trained to reach a fixed threshold at the desired time
- emergence of scaling behavior (regardless of training objectives - linear non-linear, scaling, non-scaling)
- temporal scaling explained in terms of a pair of input dependent stable FP
  - cue initialized state of network to an initial FP
  - set drove system away from FP, allowing the system to evolve to a another FP
  - speed determined by magnitude of cue input
- input: input weights, specified position of initial and terminal FP, associated with changes in the level of activity which controlled speed , no scaling
- recurrent connections: constrains imposed by recurrent weights, determined neural trajectory between FP, responsible of invariant trajectories and temporal scaling (no control of speed)
- Prediction: non-scaling signals in neuron population reflect the input that sets the speed, scaling signals correspond to the evolution of activity with desired speed
- prediction conformed, production interval can be predicted by non-scaling component of MFC activity

**Relation input and speed control**:
- eigenvalues of system near FP terminal show contraction stronger inputs
- in linear dynamical systems: contraction of eigenvalue spectrum corresponds to a systematic increase in the network's effective time constants

> action exerted by the input is equivalent to adjusting the system's effective time constant in a flexible input-dependent manner

**Two-Neuron Model**:
- adjustments of the common input in model could alter its recurrent dynamics (two regimes)
  - to relax to a single FP with specific time constant
  - to act as integreator with exceedingly long time constants
- input level can be used to create a continuum of effective time consonants
- stronger input drives neurons toward their saturating nonlinearity
  - shallower slopes - smaller derivatives, lager eff. time constnats

> single-neuron nonlinearities provides a reservoir of slopes that an input can exploit to control networks energy gradients \
> adjustments of speed were governed by the interaction of input with these nonlinearities





</details>

**Thoughts**: 

-------------------------------------------------------------------------------
### Template
<details>
<summary> summary
</summary>

details
</details>

**Thoughts**: 

-------------------------------------------------------------------------------


# ToDo Implementation
- [x] 1-2-Go task, Code that returns production times of x trials
- [x] PCA over trials and stimuli 
	- separately components over time
	- *influence of sampling actual length, upsampling?*
- [x] Experiment simulation: different ranges, keeping I
	- initial duration, stimulus, reproduction crossing, delay (or no delay with reset?) or reset to initial cond, stimulus with initI from prev. trial
- [x] **TOOL**: behavior plot, std and trials
	- extend for experiment setting
- [x] **TOOL** Script for simulating on server: save each simulation result in file 
    - [x] parallelization of simulations with parameter space


- [ ] Parameter: impact of parameter K, $\sigma$, $\tau$, delay, threshold on behavior; find regimes
	- [ ] quantify with slope and indifferent point (normalized with middle of range) and mean squared error over all trials (stimulus_lst and production list without timeouts)
	- [ ] **ADD** MSE variance bias term
	- [ ] timeouts 10% overall not in one, give nan in slope, mse,...
	- [ ] **PLOT** combination of 2 parameter and error (slope, ind point) as color 
	- [ ] **TOOL** **PLOT** function that takes 2 beh data and computes $\Delta$s (2 ranges: parameter influence on slope and indiff point $\Delta$)

- [ ] **TOOL**: function to cut u,v,y and sort according to stimulus to feed into PCA (experiment setting, parallel setting)
    - simulation returns: params, simulation, reset_lst, production_lst (with inf), timeout_index, stimulus_lst
    - [ ] create *dict* or *pandas data frame* with trial index, stimulus, production, simulation
- [ ] **TOOL**: PCA on u,v,y, across trials (mean, same sampling) and stimuli (parallel)
	- [ ] adapt for experiment simulation 
	- [ ] **PLOT** PCA in 3d and 1d
- [ ] add seed


## Model
- [ ] look at egger code implementation of experiment (2 / 3 Neuron Model)

- [ ] **Timeout**:
	- regimes over/under th, how robust?
	- when considered timeout? Plot to see of code works 
	- [x] If timeout: keep trial in exp with all updates, production is inf (or none)

- [ ] **Parameter**
	- [ ] delay
  	- wann pendelt sich I ein am Anfang (im Fehler) ein - daten?, initial I, delay relevant
  	- variable delay period?
	- 

- [ ] I zu IPI Kurve, was verÃ¤ndert sie? tau (Polstelle bei 1, VerÃ¤nderung der Regime)
  - wo sind welche ranges (mean I for range), wie stark ist das Regime ausgenutzt
  - wie kann die Dynamik verschoben werden (adaption von u), hoch oder runter fÃ¼r verschiedene ranges, oder in die breite
  - keine VerÃ¤nderung von Model! wie in Egger

- [ ] Noise in Egger Simulation module
    - is there noise on y??
    - sigma_mu: indicates fluctuation in the mean between trials added for each trial on u,v,y (self.ext = np.random.normal(0, self.sigma_mu, ntrials)
      - is this only for parallel simulation?
    - sigma_sigma: indicates the noise within each trial added for each step for u,v,y (np.random.normal(loc=0, scale=self.sigma_sigma, size=len(self.ustate)))

- [ ] Weights
  - introduce weights from u,v to y?
  - adjusting weights between I and v,w $w_{vi}$ and $w_{wi}$ instead of I

- [ ] Analytical solutions: phase plane fixed point influence of parameter
  - [ ] **PLOT** phase plane of u,v
  - mechanistically understanding of parameter to behavior
  - optimizing for K, how does K change when changing stimulus range/var?

- [ ] Population: extending units of u, v (y) with 100 cells, random weight matrix
	- [ ] how are random weight matrices chosen in u and v? differently?
	- [ ] noise in random weights; and different initials values
	- [ ] how are u and v populations connected (all to all or sub-populations)

- [ ] $\Delta$ I is only the difference of a threshold and y, can the update of I be learned?
	- [ ] Can an input I be kept in population weights of u and v? (no constant I input)


## Other
- Report Model and Figures
- markdown (latex) extension not working (shortcuts etc)

## Server
1024
- upload to server
- ssh
  - tmux
    - source .bashrc
      - conda activate time
        - python script
  - tmux attach
    - python script
- download results



