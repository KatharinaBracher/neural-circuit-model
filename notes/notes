# Reading

## Narain, Jazayeri et al. (2018): A cerebellar mechanism for learning prior distributions of time intervals
<details>
<summary> Model with 3 components, based on Bayesian estimate 
</summary>
anatomical motivated model (GC, PC, CF and DN in cerebellum) of learning prior on circuit level

Bayesian estimator assumes:
- uniform prior distribution perfectly learned by observer
- scalar variability 
- BLS estimate - expected value of posterior (vs. max likelihood - peak of posterior)
- also shown with Gaussian prior (more realistic, a learned prior can only be as precise as the likelihood, which also is Gaussian)

BLS estimator can be formulated as a deterministic nonlinear function

To learn prior model needs: 
- basis set of activity (Gaussian) that has a decay in amplitude (increase in std) to account for scalar variability
- learning rule (LTD and LTP), where LTD is suppressing PC activity at the time of set (in RSG task) and LTP as restoring force to a basis level of w, here the prior is encoded 
(should be same the other way round, in cortex)
- integrator to estimate (not necessary as information already in learned activity)

Open questions: 
- mapping from DN activity to estimate 
- reproduction of the learned interval
- population activity of basis set
</details>

## Bi and Zhou (2020): Understanding the computation of time using neural network models. 
<details>
<summary> Recurrent net, trained in a supervised fashion 
</summary>
Recurrent neural network structure: 

- 256 units
- soft plus activation
- recurrent inputs and all to all connection with learnable weights
- noise inputs
- input units depending on task, set weights
- output unit(s) with set weights, number of units depending on target

**Training**: 
- Cost function (MSE), sum over output units
- Adam optimizer

**Population activity very similar to experimental findings**:
- perceived time: stereotypical dynamic trajectories
- delay period, maintain time: attractor dynamics in a complementary monotonic coding scheme (mapping of manifold to another, different speed but same space between trajectories)
- production by scaling evolution speed.
- temporal and nontemporal information is coded in orthogonal subspaces with small mixed variance (interesting analyses)
</details>

**Thoughts**: 
- Is there are more biological motivated way to train the network? (LTD, LTP) and not so DL motivated, maybe reinforcement learning? Learning in biological plausible way
	- is the network still able to learn with a no so powerful learning rule?
- what about inh and exc weights? need restriction that a unit can only project neg. or pos.



## Egger, Le and Jazayeri (2020): A neural circuit model for human sensorimotor timing
<details>
<summary> Connecting thresholds and slopes with an inh/exc circuit 
</summary>

**absolute and predictive timing mechanisms**: 
- absolute: integrates ticks from central clock, activity increases monotonically, rate of increase does not depend on interval, output levels serve as continuous 
- predictive timing mechanism: rate of increase is adjusted so that the output reaches the same point after an anticipated interval, rate of increase has to be set

**Circuit**: 
- biological findings suggest change of rate to reach threshold 
- circuit consists of 3 units u,v,y (u, v with sigmoidal activation function) that each represent the average activity of a population of neurons
- module that translates different input $I$ to different slops: shared input to $v$ and $u$, which inhibit each other and project inh and exc respectively on $y$
  - identical shared excitatory input
  - symmetric mutual inhibition
- dynamical regimes (depend on parameter)
  - 3 FP (stable, unstable u=v, stable)
  - u (v) dominating v (u), evolve towards one of the stable FP (depending on initial conditions ramping up or ramping down)
  - leads to ramp-like behavior in y which rate is inversely proportional to input I
- initial conditions of u and v
  - mean reproduction robust, can be generalized to different dynamical regimes
- $y$ reaches threshold $y_0$, depending on slope (input $I$) the output is at different times
  - adjusting I means flexible adjusting movement initiation time (later coupling strength of I to interval time)

> speed control mechanism can be understood by analyzing system dynamics in the phase plane of u and v

**Parameter**:
- regimes of I (shifting nullclines): 
  - intermediate I, 2 FP, higher I makes the system go slower
  - low I (\<0.5), 2 FP, realtion of I and speed inverted, higher I makes the system go faster
  - high I (\>1), only 1 FP on u=v line, systems moves toward this line, requires a lower bound to be set
    - Although the circuit successfully matches IPI to the ISI in this regime, it fails to generate parallel neural trajectories in the u−v plane.
- noise level maximal 25% of input: monotonic increase of standard deviation of reproductions
  - magnitude of bias determined by noise
  - tuned to achieve a level of bias-variance trade-off that is inversely related with the inherent noise and the operative cost function
- initial delay serving as an initial estimate before any feedback
  - independent of noise level and number of stimuli
- K memory parameter
  - optimal K decreases with increasing noise
  - optimal K decreases with increasing number samples (reduced weight given to each error, allows model to integrate across inputs)
  - central role in determining the bias-variance trade-off (magnitude of bias determined by noise)

**Model that produces multiple actions and adapts I**:
- need to introduce reset mechanism to reset u and v for a new production ('new ramp'): transient pulse (1 bin)
  - neurobiological perspective: reset signal can be generated by the corollary discharge associated with the movement command or by sensory feedback triggered by the movement
  - implementation: integrating summed activity of y and tonically firing inhibitory unit y_0 into input unit, when stimulus (flash)
- need to introduce update mechanism for I to time inputs according to a stimulus
  - updating I dynamically at a rate proportional to the error signal $y_s - y_0$, $y_s$ is matched to $y_0$ at time of stimulus
- when there is a stimulus u and v are also reset
	+ we can implement the update instantaneous as $\Delta y$
- after adjusting I the motor production module outputs $y_p$ (and also reset)
	- is the motor production module necessary? the prev. module is outputting I in which the interval is already encoded

</details>

**Thoughts**:
- we don't need the slope, the interval is encoded in I (rate) and is just translated in slope with the exc/inh circuit such that high rates have slow slope and low rates have steep slope
- I functions as a threshold (absolute mechanism) and encodes the interval, slope is just the next step to reproduce the interval 

**Steps**: 
- implement and reproduce fig 3 with the update mechanism of I to encode stimulus interval 
- look at u and v as populations, is there more information in the neural trajectories?
- what about y?
- is there a biological more plausible way to update I?
- How can we learn the weights in u and v populations such that they can react on changes of stimuli ranges/ times?



## Simen, Holmes et al. (2011): A Model of Interval Timing by Neural Integration
<details>
<summary> Stochastic ramp and trigger circuit motivated by nonhomogeneous Poisson spike rate (and shot noise population model); can be reduced to a opponent poisson diffusion model 
</summary>

Stochastic ramp and trigger timing circuit
- bistable start switch with a connections to a number of clock pulse generator neurons. Number of active generator neurons determines the strength of drive to the integrator (ramp), the drive then determines the rate of ramp of the integrator. A bistable trigger switch is turned on when a fixed threshold is passed.
- 2 learning processes update the number of active units in the clock pulse generator that then directly influences the rate of the integrator 
</details>

**Thoughts**: 
<details>
<summary> Is it the same model as in Egger 2020? 
</summary>
Either the number of units is updated that determines the drive of the integrator (the more units the steeper the rate the shorter the interval); the learning rate incorporates the swap, the longer the interval, the smaller the number of active units has to be

or the input I is updated (the stronger I the smaller the slope the longer the interval); the dynamics in u,v incorporates the swap from strong input to small slope, learning is just a difference between 
</details>

- Translation of stochastic ramp and trigger timing circuit to diffusion model, with keeping parameters 
- Thresholds are implemented as bistable switches 
 
## Thurley (2016): Magnitude Estimation with Noisy Integrators Linked by an Adaptive Reference
<details>
<summary> Drift diffusion model with two processes (measurement and reproduction) and adaptive threshold (depending on memory parameter a and previous thresholds).
</summary>

**Psychophysical characteristics of magnitude estimation**: 
- Regression effect: over a range of stimuli, small stimuli are overestimated whereas large stimuli are underestimated
- Range effect: regression becomes more pronounced for ranges that comprise larger stimulus values
- Scalar variability: errors monotonically increase with the size of the stimulus
- Sequential effect: the estimate of the stimulus in a particular trial is affected by the previous trial

Effects result from an optimal strategy when noisy estimates are made about stimuli that itself depend on the statistics of the environment.

**Model**:
- Measurement (Reproduction): drift process with rate $A_m$ ($A_r$) corrupted by noise $\sigma_m$ ($\sigma_r$), dynamics described by stochastic differential equation 
  - Measurement ends with stimulus, integration differential equation yields measurement: $m_T$ -> incorporated into the threshold of reproduction
  - Measurement: drift-diffusion process that lasts a fixed time -> Gaussian distribution of level reached
  - Reproduction: drift-diffusion process with fixed threshold -> Inverse Gaussian distribution of times that it takes to reach threshold
- Threshold: depends on previous threshold (weighted by $1-a$) and measurement $m_T$ (weighted by $a$)
  - memory weight a has an immediate impact on the relation between stimulus and response
  - history of thresholds from previous trials, serving as an internal reference
- Formula for average reproduction $\bar{r}_T$ and its Bias and expected value yields full characterization of the model linking the stimuli $T$ to their reproduced values $\bar{r}_T$
- Error: $MSE_r = [(r_T-T)^2]$
  - memory a of the system can be adapted to minimize the mean squared error
- Optimizing for $a$ shows dependency on internal processing (drift ratio for m and r, internal noise $\sigma_m$ (not $\sigma_r$)) and external variability (mean and variance of stimuli).
  - Optimality predicts range and regression effects

**Fitting parameter and predictions**:
- fitting $A$, $\sigma$ to experimental data and optimize for $a$
- make predictions about $a$ based on optimality (with other parameter fitted)
- changing stimulus range predicts a different optimal $a$ - can be tested
- Predictions of the model
  - Reproduced magnitudes should depend on the stimulus distribution (Stimulus distributions with the same mean but larger variances should result in less regression)
  - Regression to the mean should depend on the discrimination abilities of the individual (internal $\sigma_m$)
  - Seldom stimuli with a low probability of occurrence and with a magnitude way below or way above the stimulus distribution, should not influence the internal reference
  - For strong regression the convergence dynamics of the reference should be much slower then for subjects showing weak regression
</details>

**Thoughts**: 
- find parameter range, what do the parameters mean? What happens when changing parameters?
- Optimality and biological relevance


## Wang, Jazayeri et al.: Flexible timing by temporal scaling of cortical responses
<details>
<summary>

Temporal scaling of activity neuron subpopulations in MFC and downstream Caudate, but not in upstream thalamic neurons. \
Recurrent neural net revealed that scaling emerges from nonlinearities in network and that degree of scaling is controlled by strength of external input. \
Introduction of two neuron model for general mechanism for translating input to the effective time constant of the dynamic.
</summary>

**PCA and SCA on neural data**
- scaling index on PCs show parts of scaling and non-scaling (-> scaling subspace)
- linear dimensionality reduction that finds axis of strongest scaling
- the higher the scaling index the higher the explained varince

**RNN**:
- input of cue with different magnitude (transient or continuous) that indicates length of interval to be reproduced and set indicating the start of the interval
- output was trained to reach a fixed threshold at the desired time
- emergence of scaling behavior (regardless of training objectives - linear non-linear, scaling, non-scaling)
- temporal scaling explained in terms of a pair of input dependent stable FP
  - cue initialized state of network to an initial FP
  - set drove system away from FP, allowing the system to evolve to a another FP
  - speed determined by magnitude of cue input
- input: input weights, specified position of initial and terminal FP, associated with changes in the level of activity which controlled speed , no scaling
- recurrent connections: constrains imposed by recurrent weights, determined neural trajectory between FP, responsible of invariant trajectories and temporal scaling (no control of speed)
- Prediction: non-scaling signals in neuron population reflect the input that sets the speed, scaling signals correspond to the evolution of activity with desired speed
- prediction conformed, production interval can be predicted by non-scaling component of MFC activity

**Relation input and speed control**:
- eigenvalues of system near FP terminal show contraction stronger inputs
- in linear dynamical systems: contraction of eigenvalue spectrum corresponds to a systematic increase in the network's effective time constants

> action exerted by the input is equivalent to adjusting the system's effective time constant in a flexible input-dependent manner

**Two-Neuron Model**:
- adjustments of the common input in model could alter its recurrent dynamics (two regimes)
  - to relax to a single FP with specific time constant
  - to act as integreator with exceedingly long time constants
- input level can be used to create a continuum of effective time consonants
- stronger input drives neurons toward their saturating nonlinearity
  - shallower slopes - smaller derivatives, lager eff. time constnats

> single-neuron nonlinearities provides a reservoir of slopes that an input can exploit to control networks energy gradients \
> adjustments of speed were governed by the interaction of input with these nonlinearities

</details>

## Egger, Jazayeri et al.: Internal models of sensorimotor integration regulate cortical dynamics

<details>
<summary> 3 epoch experiment (2 presentations of same stimulus). They show sequential update of estimate during 2 epochs (simulated motor plan during 2nd presentation), whereas in first presentation no representation of stimulus is found (set to prior)
</summary>

Internal model hypothesis: 2 populations, one that supplies constant speed command according to prior or measurement; other population that integrates this seed command and produces ramping activity to threshold

Single cell: mixed responses, 
- representation of stimulus interval in second and third epoch but not persistent, no invariant representation of stim. inerv.
- scaling: degree of scaling strongest in 3rd epoch (shown via polynomial fit); responses were temporally stretched according to stimulus interval
Population response:
- Internal model hypothesis: 
  - simulation of interval (first presentation with speed command set to prior)
  - after each flash, error in simulation is used to update speed command
  - feedback is used to update estimate sequentially; bias gets smaller for second presentation
- DMFC population activity is governed by speed-dependent predictive dynamics
  - refelcts simulated motor plan in all epochs
- DMFC population activity reflects an interval-dependent speed command
  - representation of internal estimate of stim. interv. through distance of trajectories
  - speed command may originate in thalamus (tonic thalamic input)
</details>

**Thoughts**: 



-------------------------------------------------------------------------------
### Template
<details>
<summary> summary
</summary>

details
</details>

**Thoughts**: 

-------------------------------------------------------------------------------


# ToDo Implementation
- [x] 1-2-Go task, Code that returns production times of x trials
- [x] PCA over trials and stimuli 
	- separately components over time
	- *influence of sampling actual length, upsampling?*
- [x] Experiment simulation: different ranges, keeping I
	- initial duration, stimulus, reproduction crossing, delay (or no delay with reset?) or reset to initial cond, stimulus with initI from prev. trial
- [x] **TOOL**: behavior plot, std and trials
	- extend for experiment setting
- [x] **TOOL** Script for simulating on server: save each simulation result in file 
    - [x] parallelization of simulations with parameter space
- [x] Stimulus list 
  - plot over time (oscillations?)
  - histogram uniform distribution
  - window (10-14) each stimulus needs to occur

- [x] Parameter: impact of parameter K, $\sigma$, $\tau$, delay, threshold on behavior; find regimes
	- [x] quantify with slope and indifferent point (normalized with middle of range) and mean squared error over all trials (stimulus_lst and production list without timeouts)
	- [x] **ADD** MSE as variance bias term
	- [x] timeouts 10% overall not in one, give nan in slope, mse,...
	- [x] **PLOT** combination of 2 parameter and error (slope, ind point) as color 
	- [x] **PLOT** takes 2 beh data and computes $\Delta$s (2 ranges: parameter influence on slope and indiff point $\Delta$)

- [x] **TOOL**: function to cut u,v,y and sort according to stimulus (experiment setting, parallel setting)
    - simulation returns: params, simulation, reset_lst, production_lst (with inf), timeout_index, stimulus_lst
    - [ ] create *dict* or *pandas data frame* with trial index, stimulus, production, simulation
  
- [ ] **TOOL**: PCA on u,v,y, across trials (mean, same sampling) and stimuli (parallel)
	- [ ] adapt for experiment simulation 
	- [ ] **PLOT** PCA in 3d and 1d
- [ ] add seed


## Model
- [ ] look at egger code implementation of experiment (2 / 3 Neuron Model)
- [x] Reset impulse, or reset to value? level of reset influences reproduction
- [x] Relation of I and slope of y is nonlinear
  - this is probably determining the optimal K: for longer intervals the change in slope is bigger for same I comapred to smaller intervals, so K is smaller for lager intervals
  - can we predict K for longer intervals from K for smaller intervals with this relation?

- **Parameter**
  - [x] wann pendelt sich I ein am Anfang (im Fehler) ein - daten?, initial I,
    - error over time: 2-3 trials to catch I
	- [x] delay relevant? just adding noise?
  	- variable delay period?
	- [x] for fixed tau find K for range effect
  	- for which K MSE is minimized
  	- other error? BIAS2, VAR,...?
	- [x] what influence has the *seed* on the optimal K? compute, plot systematically
	- why general overestimation
  	- [ ] use full bias 

- **Dynamic**
- [x] **PLOT** dynamics
  - what is influencing it? - parameter tau
  - [x] **PLOT** y's of experiment
  - where are the stimuli ranges situated (mean I for range), is the regime exploited?
  - can we shift the regimes? Polstelle at 1,; can we shift it up or down depending on range, expand it to the sides
- [x] **Timeout**:
	- regimes over/under th, how robust? check extremes
	- when considered timeout? Plot to see if code works 
	- [x] If timeout: keep trial in exp with all updates, production is inf (or none)
- **High-I Regime**
  - [x] Implement high I regime, with lower threshold
  - [x] what parameters have to change?
    - reset impulse (!) negativ and *10
    - initial value I over 1
    - threshold 0.1-0.2
  - explore effects
    - I, speed effect is reversed (higher I, higher speed)

- **Noise**
  - [ ] in Egger Simulation module / noise in Experiment
      - is there noise on y??
      - sigma_mu: indicates fluctuation in the mean between trials added for each trial on u,v,y (self.ext = np.random.normal(0, self.sigma_mu, ntrials)
        - is this only for parallel simulation?
      - sigma_sigma: indicates the noise within each trial added for each step for u,v,y (np.random.normal(loc=0, scale=self.sigma_sigma, size=len(self.ustate)))
  - [x] **PLOT** mean std in reproductions with different noise levels simulation
  - [x] CV std_i/mean_i should be constant over stimuli
    - mean over all stimuli should be 0.1/0.2, cut at 0.4 
    - **PLOT** sigma vs mean CV); if changing repeat 10 times

- **Weights**
  - [ ] are weights robust? (change together higher or lower)
  - introduce weights from u,v to y?
  - adjusting weights between I and v,w $w_{vi}$ and $w_{wi}$ instead of I?
  - learnable? learn update

- Analytical solutions: phase plane fixed point influence of parameter
  - [x] **PLOT** phase plane of u,v
  - mechanistically understanding of parameter to behavior
  - optimizing for K, how does K change when changing stimulus range/var?

- Population: extending units of u, v (y) with 100 cells, random weight matrix
	- [ ] how are random weight matrices chosen in u and v? differently?
	- [ ] noise in random weights; and different initials values
	- [ ] how are u and v populations connected (all to all or sub-populations)

- $\Delta$ I 
  - is only the difference of a threshold and y, can the update of I be learned?
	- [ ] Can an input I be kept in population weights of u and v? (no constant I input)

- Translation from fixed slope changing threshold to fixed threshold changing slope
  - same predictions? Mathematics 

- **E(stim)/var(stim)**
  - how does optimal K change for different E/var
  - [ ] try simulation (for tau 140, opt K), for different ranges (broader, middle, bigger, all), Randphänomene, predictions?
    - all: higher var (all 13) -> steeper  [400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000]
    - short_few: fewer in range (3/7) -> steeper  [400, 550, 700]
    - all_few: [400, 500, 600, 700, 800, 900, 1000]
    - mid: higher E (E=700) -> [550, 600, 650, 700, 750, 800, 850]
  - [ ] plot E/var vs. K or slope




## Comments
- we need K to achieve the range effect (slopes)
- K is consistent with Bayes, higher weight update for short range, lower weight update for long range
- the reset impulse has an influence on estimate (activity level where it starts to ramp changes)

Optimizing behaviour
- **MSE** opt. K
  - too shallow slope, needs to be larger to be plausible slope respecially for short range
- **Slope** Data opt. K
  - larger K especially for long range compared to MSE opt. K
  - but if we look at global minimum MSE K fits better to MSE (in intermedaite)
  - to avoid general underestimation in large range -> need higher K (similar to short range), much higher tau and higher thresold to achieve overestmation
- **BIAS2** opt K higher, similar between s and l
  - too steep slopes, K needs to be smaller
  - not optimizing for bias2 only
- **VAR** is pulling K to smaller values -> but flat curve?
- **BIAS**: 0 bias difficult to achive for long range, need much larger 


## Other
- clean code
- document code
- write down methods in report
- markdown (latex) extension not working (shortcuts etc)
- double function clean
- highI functinoality

## Server
1024
- upload to server
- ssh
  - tmux
    - source .bashrc
      - conda activate time
        - python script
  - tmux attach
    - python script
- download results


## Pitch
- We experience time, but there is a key difference to the other senses: we don't have a specialized sensory organ to sense time. 
- So how does the brain compute time? What are the neural mechanisms is unknown.
- Here I am modeling time reproduction experiments. The model is a simple circuit model by the Jazayeri lab. It is based on the phenomenon of scaling, observed both at single cell and population activity level.
- In experiment simulations with this model we try to reproduce typical behavioral effects that are found in time reproduction experiments. 

Scaling in MFC
- Medial frontal cortex (MFC): Scaling subspace: correspond to the evolution of activity with desired speed
- Nonscaling subspace: reflect input that sets the speed 
- scaling may originate within MFC or other cortical circuits projecting to MFC
- Input is adjusting the system’s effective time constant in a flexible input-dependent manner

## Intro
- Magnitude estimation
- model für magnitude estimation
- Scaling and error update Egger and co
- Model in Egger, how it is used
- Model is used for decision making
- research question: can the model be applied to more realistic experiment simulations? How much of the effects in real behavior does it capture?
- andere effecte ranges

## Discussion
- Thurley 2016: Trial-by-trial update rules have been used by others to explain aspects of magnitude estimation (Hellström, 2003; Dyjas et al., 2012; Bausenhart et al., 2014) and are also at the core of the Bayesian model by Petzschner and Glasauer (2011), where such updating is used to adjust prior knowledge about the stimulus distribution. 
- optimization 
- Bayesian model? statistisches model
- mechanismus uaf populations ebene, raten basiert
- generell modelierung von Zeitrepro
- entschiedung und Zeit zusammenhang (model basierent auf desition making)
- remapping (=Anpassung des dynamischen Regimes for current experiment, rescaling x axis) var: vorhersage des models ist var keinen Einfluss, in Daten zwei Möglichkeiten -> porpose experiment change var further, switch? when switch? to take further information into account
  model needs additional mechanism to adjust regime to picture the data possibilities
- model in one regime -> range effect, breaks down, needs remapping of axis to higher (ms to s)
  this is also in the Data: range effect ms vs s, Anpassungsmechanismus
- Vorhersage des Models: so ist das Verhalten
  Data: manche machen var so manche so
  -> model anpassen damit es abbildet, in daten auch Anpassungsmechanismus (gegeben dass model beschreibt was Gehrin macht)





